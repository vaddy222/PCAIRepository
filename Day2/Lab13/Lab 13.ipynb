{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb011acf-9459-4406-af3b-b299829f1f98",
   "metadata": {},
   "source": [
    "## Exploring Multimodal Capabilities of LLaVA\n",
    "\n",
    "Multimodal models are AI systems that process and integrate multiple types of data, such as text, images, audio, or video, to generate richer and more context-aware outputs. These models enhance understanding by leveraging complementary information across different modalities, improving tasks like image captioning, language translation, and interactive AI applications.\n",
    "\n",
    "Here, we demonstrate how to make the model generate image descriptions based on an input image. \n",
    "\n",
    "We use LLaVA, which is a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6387d-8a55-4925-9bd9-69b9bd1b94d2",
   "metadata": {},
   "source": [
    "## Lab Description:\n",
    "\n",
    "This lab explores the multimodal capabilities of **LLaVA (Large Language and Vision Assistant)** by leveraging its ability to process and generate textual descriptions from images. Participants will load a pretrained LLaVA model and use it to generate image captions, analyze visual content, and interpret images. Through hands-on exercises, learners will gain insights into how multimodal models integrate visual and textual information to enhance AI understanding. \n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "- Understand Multimodal AI and LLaVAâ€™s Capabilities.\n",
    "- Generate and Analyze Image Descriptions.\n",
    "- Explore the Role of Multimodal Models in AI-Assisted Image Interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04be992-0b05-42bd-aa17-640239e91240",
   "metadata": {},
   "source": [
    "### Libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65939a76-1d30-45a5-8e72-bd56fa9880e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd47e991-e77a-4d66-9cf9-01baf12ba800",
   "metadata": {},
   "source": [
    "### Passing Multimodal data into the model\n",
    "\n",
    "The most common way to pass multimodal data like images to a model is to pass it as a byte string. We take a file path, then read the binary content of the image file and then encode it using b64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26f4832-33fa-4681-91ba-617637dc6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Define the path to the local image file\n",
    "file_path = \"./winter.jpg\"\n",
    "\n",
    "# Open the file in binary mode and read the content\n",
    "with open(file_path, \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb5cee-b772-49cf-bd00-9257df447792",
   "metadata": {},
   "source": [
    "We then load `llava:7b` / `llava:13b` from ollama using langchain's `ChatOllama`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db5382db-04e5-4a10-93f8-4573bbb06c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llava:13b\", base_url=\"http://10.79.253.112:11434\")  #load the multimodal model from ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc4e12-91a6-4648-a77e-da9d9436f67b",
   "metadata": {},
   "source": [
    "We use `messages` to converse with the the model. We have a `HumanMessage` object which contains a `content`. In this `content` we have our text prompt (based on which the model will provide answers after analysing the image), and then we have the byte string we generated for the image. \n",
    "\n",
    "<img src=\"./winter.jpg\" alt=\"Winter\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098b74de-8c0b-49c7-ad20-ed7fc496ab1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The image depicts a snowy, winter scene. A log cabin is nestled amidst tall coniferous trees that are heavily laden with snow, suggesting the recent fall and current cold conditions. The sky appears to be clear and dark blue, which might indicate either early morning or late evening, contributing to the serene atmosphere. The ground is covered in a blanket of freshly fallen snow, indicating the photo was likely taken after a heavy snowfall, and it's still actively snowing.\n",
       "\n",
       "The cabin has a warm glow emanating from its windows, which adds to the cozy and inviting ambiance despite the cold weather outside. The presence of a chimney suggests that the cabin is equipped with a fireplace for warmth, further emphasizing the need for shelter in such conditions.\n",
       "\n",
       "Overall, the image conveys a tranquil winter scene with a strong emphasis on the contrast between the warm interior of the cabin and the cold, snowy environment outside. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = HumanMessage(                              \n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Analyse the weather and atmospheric condition from the given image\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f1bbf-080f-420f-b604-bae3e05ed098",
   "metadata": {},
   "source": [
    "### Passing Multiple Images\n",
    "\n",
    "We can also pass multiple images at the same time. For this, we will first have to load two images, and then generate byte strings for both the images. Once we have both strings, we can provide input to the model using the same `messages` from langchain. \n",
    "\n",
    "<img src=\"./winter.jpg\" alt=\"Winter\" width=\"600\" />\n",
    "<img src=\"./sunny.jpg\" alt=\"Sunny\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e4ce11-edad-43be-a23c-ff658bb45b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Define the path to the local image file\n",
    "file_path1 = \"./winter.jpg\"\n",
    "file_path2 = \"./sunny.jpg\"\n",
    "\n",
    "# Open the file in binary mode and read the content\n",
    "with open(file_path1, \"rb\") as image_file:\n",
    "    image_data1 = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "with open(file_path2, \"rb\") as image_file:\n",
    "    image_data2 = base64.b64encode(image_file.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d341dd4-e066-49c4-8b54-b39107f914ea",
   "metadata": {},
   "source": [
    "We added an additional entry for passing the second image to the model. We changed the prompt to make the model analyse and compare both the images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a59fa8-6e06-4733-b0f2-4ab635de989e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Both images depict a serene coastal setting, specifically focusing on the area around a cabin by the beach during winter. Here's an analysis of the weather and atmospheric conditions:\n",
       "\n",
       "1. Sunlight and Skies:\n",
       "   - The top image shows a clear sky with no visible clouds, indicating good visibility and likely low humidity. This suggests it might be a cold but sunny day.\n",
       "   - The bottom image also has a clear blue sky without any clouds, which continues to indicate sunshine and fair weather conditions.\n",
       "\n",
       "2. Temperature:\n",
       "   - The absence of visible snow or ice in the lower image indicates that while it's winter, the temperature might be above freezing, at least during the day, given that the beach is accessible without any signs of heavy frost or accumulated snow.\n",
       "\n",
       "3. Snow and Frost:\n",
       "   - In both images, there are no signs of heavy snowfall. The only snow visible in the lower image appears to be on the ground around the cabin, suggesting recent light snowfall that has not entirely covered the landscape.\n",
       "\n",
       "4. Wind Conditions:\n",
       "   - Both scenes show calm seas and minimal wind, which gives a tranquil atmosphere to both images.\n",
       "\n",
       "Overall, the weather in both images appears to be fairly similar with clear skies, sunshine, and minimal snow or ice. The main difference seems to be the time of day due to the positioning of the sun. The top image shows the sun at a lower angle, which is consistent with either morning or evening light, whereas the bottom image has the sun at a higher angle, indicating midday. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Analyse and compare weather and atmospheric conditions of the 2 images.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data1}\"}},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data2}\"}},\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad90ef-c017-4d89-b1cd-57b1d78a7dab",
   "metadata": {},
   "source": [
    "The model generated a comparitive description of both the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d6d0b-98d2-4f51-aa84-8ce59a5e1b19",
   "metadata": {},
   "source": [
    "## Analyzing Tables\n",
    "\n",
    "Let us now try to analyze data from a graphical table containing information about Intel Processors used in HPE Proliant DL380a Gen12. \n",
    "\n",
    "<img src=\"./table.jpg\" alt=\"table\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f8485-4995-4efb-88a3-2915ac29d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Define the path to the local image file\n",
    "file_path = \"./table.jpg\"\n",
    "\n",
    "# Open the file in binary mode and read the content\n",
    "with open(file_path, \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e574ae84-777e-448f-a22c-f66b21530525",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(                              \n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Analyze the given Image of the table\"},\n",
    "        {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "response = model.invoke([message])\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cafc8-a9fa-4284-ac9e-82112c3a4043",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    <img src=\"logo.png\" alt=\"flow\" width=\"150\" height=\"100\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ec449-9c54-40bb-92d5-426ba5881531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
